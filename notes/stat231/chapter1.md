# Chapter 1: Introduction to Statistical Science

## 1.1 Statistical Sciences

Statistical Science are concerned with all aspects of **empirical studies** including problem formulation, planning of an experiment, data collection, analysis of the data, and the conclusion that can be made. An empirical study is one in which we learn by observation or experiment. A key feature of such studies is that there is usually uncertainty in the conclusions. An important task in empirical studies is to quantify this uncertainty.

Empirical studies deal with **populations** and **processes**; both of which are collections of individual **units**. In order to increase our knowledge about a process, we examine a **sample** of units generated by the process. To study a population of units we examine a sample of units carefully selected from that population. Two challenges arise since we only see a sample from the process or population and not all of the units are the same. Statistical Sciences deal both with the study of variability in processes and populations, and with good (that is, informative, cost-effective) ways to collect and analyze data about such processes.

## 1.2 Collecting Data

A **population** is a collection of **units**. A **process** is a system by which units are produced. A key feature of processes is that they usually occur over time whereas populations are often static.

We pose questions about populations (or processes) by defining **variates** for the units which are characteristics of the units. Variates can be different types. Variates such as height and weight of a person, lifetime or an electrical component, and time until recurrence of disease after medical treatment are all examples of **continuous** or **measured** variates. Variates such as the number of defective smart phones sold by a particular company in a week, the number of deaths in a year on a dangerous highway or the number of damaged pixels in a monitor are all examples of **discrete** variates.

Variates such as hair color, university program or martial status are examples of **categorical** variates since these variates do not take on numerical values. Sometimes, to facilitate the analysis of the data, we might redefine the variate of intersect to be 1 if present and 0 if absent. In such a case we would now call the variate a discrete variate. Since the variate only takes on values 0 or 1 such a variate is also often called a **binary** variate.

If a variate classifies a unit by size then this is an example of a categorical variate. However since this categorical variate has a natural ordering, it is also called an **ordinal** variate. 

Statistical Sciences stress the importance of obtaining data that will be objective and provide maximal information at a reasonable cost. There are three broad approaches:

*	**Sample Surveys**. The object of many studies is to learn about a finite population. In this case information about the population may be obtained by selecting a "representative" sample of units from the population and determining the variate of interest for each unit in the sample. Obtaining such a sample can be challenging and expensive. In a survey sample the variates of interest are most often collected using a questionnaire. Sample survey are widely used in government statistical studies, economics, marketing, public opinion polls, sociology, quality assurance and other areas.
*	**Observational Studies**. An observational study is one in which data are collected about a process or population without any attempt to change the value of one or more variates for the sampled units. A distinction between a sample survey and an observational study is that for observational studies the population of interest is usually infinite or conceptual.
*	**Experiments or Experimental Studies**. An experiment is a study in which the experimenter intervenes and changes or sets the values of one or more variates for the units in the sample.

## 1.3 Data Summaries

There are two classes of summaries: graphical and numerical. Suppose that data on a variate $y$ is collected for $n$ units in a population or process. By convention, we label the units as $1,2,...,n$ and denote their respective $y$-value as $y_1,y_2,...,y_n$. We might also collect data on a second variate $x$ for each unit, and we would denote the values as $x_1,x_2,...,x_n$. We refer to $n$ as the **sample size** and to $\{x_1,x_2,...,x_n\}$, $\{y_1,y_2,...,y_n\}$ or $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$ as data sets.

#### Numerical Summaries

These summaries fall generally into three categories: measures of location (mean, median, and mode), measure of variability or dispersion (variance, range, and interquartile range), and measures of shape (skewness and kurtosis).

#### 1. Measure of locations:

*	The **sample mean** also call the sample average: $\bar{y}=\frac{1}{n}\sum_{i=1}^{n}y_i$.
*	The **sample median** $\hat{m}$ or the middle value when $n$ is odd and the sample is ordered from smallest to largest, and the average of the two middle values when $n$ is even. Since the median is less affected by a few extreme observations it is a more robust measure of location.
*	The **sample mode**, or the value of $y$ which appears in the sample with the highest frequency (not necessarily unique).

#### 2. Measure of dispersion or variability:

*	The **sample variance**:
$$s^2=\frac{1}{n-1}\sum_{i=1}^{n}(y_i-\bar{y})^2=\frac{1}{n}\left[\sum_{n=1}^{n}y_i^2-n(\bar{y})^2\right]$$
and the **sample standard deviation**: $s=\sqrt{s^2}$.
*	The **range** $=y_{(n)}-y_{(1)}$ where $y_{(n)}=max(y_1,y_2,...,y_n)$ and $y_{(1)}=min(y_1,y_2,...,y_n)$.
*	The **interquartile range IQR** which is described below.

The sample variance and the sample standard deviation measure the variability or spread of the variate values in a data set. The units for standard deviation, range and interquartile range are the same as for the original variate.

#### 3. Measures of shape:

Measures of shape generally indicate how the data, in terms of a relative frequency histogram, differ from the Normal bell-shaped curve, for example whether one tail of the relative frequency histogram is substantially larger than the other so the histogram is asymmetric, or whether both tails of the relative frequency histogram are large so the data are more prone to extreme values than data from a Normal distribution.

*	The **sample skewness**
$$g_1=\frac{\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^3}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2\right]^{3/2}}$$

	is a measure of the (lack of) symmetry in the data.

	When the relative frequency histogram of the data is approximately symmetric then there is an approximately equal balance between the positive and negative values in the sum $\sum_{i=1}^{n}(y_i-\bar{y})^3$ and this results in a value for the skewness that is approximately zero. 

	If the relative frequency histogram of the data has a long right tail, then the positive values of $(y_i-\bar{y})^3$ dominate the negative values in the sum and the value of the skewness will be positive. 

	Similarly if the relative frequency histogram of the data has a long left tail then the value of the skewness will be negative.

*	The **sample kurtosis**
$$g_2=\frac{\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^4}{\left[\frac{1}{n}\sum_{i=1}^{n}(y_i-\bar{y})^2\right]^2}$$

	measures the heaviness of the tails and the peakedness of the data relative to data that are Normally distributed. For the Normal distribution the kurtosis is equal to 3.

	Since the term $(y_i-\bar{y})^4$ is always positive, the kurtosis is always positive and values greater than three indicate heaver tails (and a more peaked center) than data that are Normally distributed.

#### Sample Quantiles and Percentiles

For $0<p<1$, the $p$-th quantile (also called the $100p$-th percentile) is a value such that approximately a fraction $p$ of the $y$ values in the data set are less than $q(p)$ and roughly $1-p$ are greater. Depending on the size of the data set, quantiles are not uniquely defined for all values of $p$. There are different conventions for defining quantiles in these cases. If the sample size is large, the differences in the quantiles based on the various definitions are small.

#### Definition 1

Let $\{y_{(1)},...,y_{(n)}\}$ where $y_{(1)}\leq\cdots\leq y_{(n)}$ be the order statistic for the data set $\{y_1,...,y_n\}$. The $p$-th **sample quantile** is a value, call it $q(p)$, determined as follows:

*	Let $m=(n+1)p$ where $n$ is the sample size
*	If $m$ is an integer between $1$ and $n$ then $q(p)=y_{(m)}$ which is the $m$-th largest value in the data set.
*	If $m$ is not an integer but $1<m<n$ when determine the closest integer $j$ such that $j<m<j+1$ and take $q(p)=\frac{1}{2}[y_{(j)}+y_{(j+1)}]$.

The quantiles $q(0.25)$, $q(0.5)$, $q(0.75)$ are often used to summarize a data set and are given special names.

#### Definition 2

The **quantiles** $q(0.25)$, $q(0.5)$, $q(0.75)$ are called the lower or first quartile, the median, and the upper or third quartile respectively.

#### Definition 3

The **interquartile range is IQR** $=q(0.75)-q(0.25)$.

Since the interquartile range is less affected by a few extreme observations, it is a more robust measure of variability as compared to the sample standard deviation.

#### Definition 4

The **five number summary** of a data set consists of the smallest observation, the lower quartile, the median, the upper quartile and the largest value, that is, the five values $y_{(1)}, q(0.25), q(0.5), q(0.75), y_{(n)}$.

The five number summary provides a concise numerical summary of a data set which provides information about the location (through the median), the spread (through the lower and upper quartiles) and the range (through the minimum and maximum values).

#### Sample Correlation

So far we have looked only at graphical summaries of a data set. Often we have bivariate data of the form $\{(x_1,y_1),...,(x_n,y_n)\}$. A numerical summary of such data is the sample correlation.

#### Definition 5

The **sample correlation**, denoted by $r$ is
$$r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$$
where
$$S_{xx}=\sum_{i=1}^{n}(x_i-\bar{x})^2=\sum_{i=1}^{n}x_i^2-n(\bar{x})^2$$
$$S_{xy}=\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^{n}x_iy_i-n\bar{x}\bar{y}$$
and
$$S_{yy}=\sum_{i=1}^{n}(y_i-\bar{y})^2=\sum_{i=1}^{n}y_i^2-n(\bar{y})^2$$

The sample correlation, which takes on values between $-1$ and $1$, is a measure of the linear relationship between the two variate $x$ and $y$. If the value of $r$ is close to 1 then we say that there is a strong positive linear relationship between the two variates while if the value of $r$ is close to $-1$ then we say that there is a strong negative linear relationship between the two variates. If the value of $r$ is close to $0$ then we say that there is no linear relationship between the two variates.

#### Relative Risk

Recall that categorical variates consist of group of category names that do not necessary have any ordering. If two variates of interest in a study are categorical variates then it does not make sense to use sample correlation as a measure of the relationship between the two variates.

#### Definition 6

For categorical data, the **relative risk** of event $A$ in group $B$ as compared to group $\bar{B}$ is
$$relative \quad risk = \frac{y_{11}/(y_{11}+y_{12})}{y_{21}/(y_{21}+y_{22})}$$

#### Graphical Summaries

We consider several types of plots for a data set $\{y_1,...,y_2\}$ and one type of plot for a data set $\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$.

#### Frequency histogram

Consider measurements $\{y_1,...,y_2\}$ on a variate $y$. Partition the range of $y$ into $k$ non-overlapping intervals $I_j=[a_{j-1},a_j), j=1,2,...,k$ and then calculate for $j=1,2,...,k$. $f_j=$ number of values from $\{y_1,...,y_2\}$ that are in $I_j$.

The $f_j$ are called the **observed frequencies** for $I_1,...,I_k$; note that $\sum_{j=1}^{k} k_j=n$. A **histogram** is a graph in which a rectangle is placed above each interval; the height of the rectangle for $I_j$ is chosen so that the area of the rectangle is proportional to $f_j$. Two main types of frequency histogram are:

1.	a "standard" frequency histogram where the intervals $I_j$ are of equal length. The height of the rectangle for $I_j$ is the frequency $f_j$ or **relative frequency** $f_j/n$. This type of histogram is similar to a bar chart.
2.	a "relative" frequency histogram, where the interval $I_j=[a_{j-1},a_j)$ may or may not be of equal length. The height of the rectangle for $I_j$ is chosen so that its area equals $f_j/n$, that is, the height of the rectangle for $I_j$ is equal to
$$\frac{f_j/n}{(a_j-a_{j-1})}$$

If intervals of equal length are used then a standard frequency histogram and a relative frequency histogram look identical except for the labeling of the vertical axis. For a relative frequency histogram the sum of the areas of the rectangles is equal to one. If we wish to compare two data sets which have different sample sizes then a relative frequency histogram must be used. If we wish to superimpose a probability density function on a frequency histogram to see how well the data fit the model then a relative frequency histogram must always be used.

To construct a frequency histogram, the number and location of the intervals must be chosen. The intervals are typically selected so that there are ten to fifteen intervals and each interval contains at least one $y$-value from the sample.

#### Empirical Cumulative Distribution Functions

Another way to portray the values of a variate $\{y_1,...,y_2\}$ is to determine the proportion of values in the set which are smaller than any given value. This is called the **empirical cumulative distribution function** or e.c.d.f. and is defined by
$$\hat{F}(y)=\frac{number \quad of \quad values \quad in \quad \{y_1,...,y_2\} \quad which \quad are \quad \leq y}{n}$$

To construct $\hat{F}(y)$, it is convenient to first order the $y_i$'s to give the ordered values. Then we note that $\hat{F}(y)$ is a step function with a jump at each of the ordered observed values.

